 Open
chore: add dev-setup, mcp mock, validator, smoke test, and docs
#32
iHelper30 wants to merge 3 commits into ax-platform:main from iHelper30:challenge/ax-clean-install 
+379 âˆ’156 
 Conversation 16
 Commits 3
 Checks 2
 Files changed 9
Conversation
@iHelper30
iHelper30
commented
2 days ago
aX Agent Studio - Developer Demo
This branch (challenge/ax-clean-install) implements a reproducible, one-command developer environment with a local MCP mock server and smoke testing harness.

ðŸš€ One-Command Start
To start the entire environment (virtual env creation, dependency installation, and dashboard startup):

.\scripts\dev-setup.ps1
This script will:

Check for/create .venv and install pip.
Install dependencies from pyproject.toml or requirements.txt.
Set PYTHONUTF8=1 to avoid Windows encoding issues.
Start the Agent Studio Dashboard on http://127.0.0.1:8000.
ðŸ§ª Running the Smoke Test
To verify agent stability, heartbeats, and message flow without external dependencies:

Start the MCP Mock Server (in a separate terminal):

.\.venv\Scripts\python.exe scripts/mcp_mock.py
Register Agents (populates the mock workspace):

.\.venv\Scripts\python.exe scripts/register_agents.py
Run the Load/Smoke Test:

# Run 5 agents for 30 seconds with 5-second heartbeats
.\.venv\Scripts\python.exe tests/stability/load_test.py --agents 5 --duration 30 --heartbeat 5
ðŸ› ï¸ Features Added
scripts/dev-setup.ps1: Automated setup and startup script.
scripts/mcp_mock.py: Minimal HTTP server mocking MCP endpoints for deterministic testing.
scripts/validate_configs.py: Validates agent JSON configurations against required schema.
src/ax_agent_studio/llm_factory.py: Added fallback stub to prevent crashes when API keys are missing.
tests/stability/load_test.py: Harness for verifying system stability under load.
ðŸ“‹ Verification Results
Venv: Python 3.14.0, pip 25.3
Config Validation: All agent configs valid.
Smoke Test: 100% success rate on 5-agent run.
@iHelper30
chore: add dev-setup, mcp mock, validator, smoke test, and docs
3c93bc0
@chatgpt-codex-connector
chatgpt-codex-connector bot
commented
2 days ago
Codex usage limits have been reached for code reviews. Please check with the admins of this repo to increase the limits by adding credits.
Credits must be used to enable repository wide code reviews.

New changes since you last viewed
iHelper30 and others added 2 commits 2 days ago
@iHelper30
chore: polish demo artifacts (encoding fix, robust startup, extended â€¦ 
b8c0c6d
@madtank
Merge branch 'main' into challenge/ax-clean-install
c5cc22f
@madtank madtank requested a review from Copilot 2 days ago
Copilot started reviewing on behalf of madtank 2 days ago
Copilot finished reviewing on behalf of madtank 2 days ago
Copilot AI reviewed 2 days ago
Copilot AI
left a comment
Pull request overview
This PR introduces a comprehensive developer environment setup with automated tooling for the aX Agent Studio project, enabling reproducible one-command installation and smoke testing capabilities.

Key Changes:

Automated developer setup via PowerShell script with virtual environment creation and dependency management
Mock MCP server implementation and stability testing harness for offline development and validation
Simplified LLM factory with fallback stub behavior to prevent crashes when API keys are missing
Reviewed changes
Copilot reviewed 9 out of 9 changed files in this pull request and generated 14 comments.

Show a summary per file
Comments suppressed due to low confidence (2)
.env.example:174

Several model versions listed do not exist or are not valid as of January 2025:
claude-sonnet-4-5, claude-haiku-4-5, claude-opus-4-5 - Anthropic's Claude 4 series does not exist; the latest is Claude 3.5 (e.g., claude-3-5-sonnet-20241022)
gpt-5, gpt-5-mini, gpt-5-nano, o4-mini - OpenAI has not released GPT-5; the latest is GPT-4 series (e.g., gpt-4-turbo, gpt-4o)
gemini-2.5-pro, gemini-2.5-flash, gemini-2.5-flash-lite - Google Gemini 2.5 does not exist; the latest is Gemini 2.0 (e.g., gemini-2.0-flash)
us.anthropic.claude-sonnet-4-v1:0, us.anthropic.claude-haiku-4-v1:0 - Bedrock Claude 4 models do not exist
Please update these to valid model names that exist in the respective APIs.

# Anthropic: claude-sonnet-4-5, claude-haiku-4-5, claude-opus-4-5
# OpenAI: gpt-5, gpt-5-mini, gpt-5-nano, o4-mini
# Google: gemini-2.5-pro, gemini-2.5-flash, gemini-2.5-flash-lite
# Bedrock: us.anthropic.claude-sonnet-4-v1:0, us.anthropic.claude-haiku-4-v1:0
scripts/mcp_mock.py:28

Variable post_data is not used.
        post_data = self.rfile.read(content_length)
ðŸ’¡ Add Copilot custom instructions for smarter, more guided reviews. Learn how to get started.

src/ax_agent_studio/llm_factory.py
Comment on lines +27 to +28
    def create(provider, model):
        return generate_with_fallback
Copilot AI
2 days ago
The create method signature doesn't match the expected factory pattern. It accepts provider and model parameters but doesn't use them, and returns the generate_with_fallback function directly instead of a proper LLM client instance. This breaks the factory interface that the rest of the codebase likely expects.

Copilot uses AI. Check for mistakes.
@iHelper30	Reply...
scripts/mcp_mock.py

    def do_POST(self):
        content_length = int(self.headers['Content-Length'])
        post_data = self.rfile.read(content_length)
Copilot AI
2 days ago
The POST handler reads the request body but never uses or validates post_data. If this data is intentionally ignored for the mock, consider adding a comment explaining this. Otherwise, consider logging or validating the payload for better debugging support during testing.

Suggested change
        post_data = self.rfile.read(content_length)
        post_data = self.rfile.read(content_length)
        # Log the received POST payload for debugging purposes
        try:
            print("Received POST payload:", post_data.decode('utf-8'))
        except Exception as e:
            print("Failed to decode POST payload:", e)
Copilot uses AI. Check for mistakes.
@iHelper30	Reply...
.env.example
# Leave empty to use model defaults, or specify a prompt file from configs/prompts/
# Examples: helpful_assistant, creative_writer, code_reviewer
# DEFAULT_SYSTEM_PROMPT=
# DEFAULT_SYSTEMCP_REMOTE_TOKEN=
Copilot AI
2 days ago
Typo in comment: "DEFAULT_SYSTEMCP_REMOTE_TOKEN" should be "DEFAULT_SYSTEM_PROMPT" based on the context and surrounding comments.

Suggested change
# DEFAULT_SYSTEMCP_REMOTE_TOKEN=
# DEFAULT_SYSTEM_PROMPT=
Copilot uses AI. Check for mistakes.
@iHelper30	Reply...
tests/stability/load_test.py
            "seq": seq,
            "ts": time.time()
        }
        response = requests.post(f"{MOCK_URL}/workspaces/demo/messages", json=payload)
Copilot AI
2 days ago
The HTTP request lacks a timeout parameter, which could cause the test to hang indefinitely if the server is unresponsive. Add a timeout (e.g., timeout=5) to ensure the test fails fast and doesn't block forever.

Suggested change
        response = requests.post(f"{MOCK_URL}/workspaces/demo/messages", json=payload)
        response = requests.post(f"{MOCK_URL}/workspaces/demo/messages", json=payload, timeout=5)
Copilot uses AI. Check for mistakes.
@iHelper30	Reply...
scripts/mcp_mock.py
            self.end_headers()
            response = {
                "messages": [
                    {"seq": 1, "ts": time.time(), "payload": {"agent": "mock_agent", "text": "Hello form mock!"}}
Copilot AI
2 days ago
Spelling error: "form" should be "from" in the mock message text.

Suggested change
                    {"seq": 1, "ts": time.time(), "payload": {"agent": "mock_agent", "text": "Hello form mock!"}}
                    {"seq": 1, "ts": time.time(), "payload": {"agent": "mock_agent", "text": "Hello from mock!"}}
Copilot uses AI. Check for mistakes.
@iHelper30	Reply...
src/ax_agent_studio/llm_factory.py
        # In a real implementation, this would call the actual LLM API.
        # For this demo/stub, we check if we have credentials, otherwise fallback.
        if os.getenv("GOOGLE_API_KEY") or os.getenv("OPENAI_API_KEY"):
             # Placeholder for actual call
Copilot AI
2 days ago
Inconsistent indentation: this line has an extra space at the beginning (5 spaces instead of 4). Should align with the surrounding code.

Suggested change
             # Placeholder for actual call
        # Placeholder for actual call
Copilot uses AI. Check for mistakes.
@iHelper30	Reply...
tests/stability/load_test.py

        # Verify messages were received
        try:
            r = requests.get(f"{MOCK_URL}/workspaces/demo/messages?since_seq=0")
Copilot AI
2 days ago
The HTTP request lacks a timeout parameter, which could cause the verification step to hang indefinitely if the server is unresponsive. Add a timeout (e.g., timeout=5) to ensure the test completes in a reasonable time.

Suggested change
            r = requests.get(f"{MOCK_URL}/workspaces/demo/messages?since_seq=0")
            r = requests.get(f"{MOCK_URL}/workspaces/demo/messages?since_seq=0", timeout=5)
Copilot uses AI. Check for mistakes.
@iHelper30	Reply...
scripts/register_agents.py
                "agent_id": agent['name'],
                "type": "registration"
            }
            response = requests.post(f"{MOCK_URL}/workspaces/demo/messages", json=payload)
Copilot AI
2 days ago
The HTTP request lacks a timeout parameter, which could cause the script to hang indefinitely if the mock server is unresponsive. Add a timeout (e.g., timeout=5) to ensure the script fails fast.

Suggested change
            response = requests.post(f"{MOCK_URL}/workspaces/demo/messages", json=payload)
            response = requests.post(f"{MOCK_URL}/workspaces/demo/messages", json=payload, timeout=5)
Copilot uses AI. Check for mistakes.
@iHelper30	Reply...
src/ax_agent_studio/llm_factory.py
Comment on lines +13 to +15
        if os.getenv("GOOGLE_API_KEY") or os.getenv("OPENAI_API_KEY"):
             # Placeholder for actual call
             pass
Copilot AI
2 days ago
The condition checks for API keys but then does nothing with the result. If API keys exist, the code falls through to the fallback response without attempting any actual LLM call. This means the function always returns stub responses, even when credentials are available. Either implement the actual LLM call or remove the credential check if this is intentionally a stub-only implementation.

Copilot uses AI. Check for mistakes.
@iHelper30	Reply...
.env.example
Comment on lines +83 to 172
# Default model for the selected provider
# Anthropic: claude-sonnet-4-5, claude-haiku-4-5, claude-opus-4-5
# OpenAI: gpt-5, gpt-5-mini, gpt-5-nano, o4-mini
# Google: gemini-2.5-pro, gemini-2.5-flash, gemini-2.5-flash-lite
# Bedrock: us.anthropic.claude-sonnet-4-v1:0, us.anthropic.claude-haiku-4-v1:0
# aX Agent Studio - Environment Variables
# Copy this file to .env and fill in your API keys

# =============================================================================
# LLM Provider API Keys
# =============================================================================

# Google Gemini API Key
# Get yours at: https://ai.google.dev/
GOOGLE_API_KEY=your_google_api_key_here

# Anthropic Claude API Key
# Get yours at: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# =============================================================================
# Agent Framework Preferences
# =============================================================================

# Default agent type for dashboard (echo, ollama, langgraph, claude_agent_sdk)
# This sets the initial selection when you open the dashboard
DEFAULT_AGENT_TYPE=claude_agent_sdk

# Claude Agent SDK: Use subscription instead of API key billing
# Set to 'true' to use Claude Pro/Max subscription via Claude CLI credentials
# Requires: claude login (one-time setup)
# Only affects Claude Agent SDK monitor - other monitors still use ANTHROPIC_API_KEY
USE_CLAUDE_SUBSCRIPTION=true

# =============================================================================
# Additional LLM Provider API Keys
# =============================================================================

# OpenAI API Key
# Get yours at: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# AWS Bedrock (uses AWS credentials)
# Either set these or use ~/.aws/credentials
# AWS_ACCESS_KEY_ID=your_aws_access_key
# AWS_SECRET_ACCESS_KEY=your_aws_secret_key
# AWS_REGION=us-east-1

# Set to true to enable Bedrock even without explicit AWS keys (uses local AWS config)
# AWS_BEDROCK_ENABLED=true

# =============================================================================
# Ollama Configuration (Local Models)
# =============================================================================

# Ollama runs locally, no API key needed
# Make sure Ollama is running: https://ollama.ai
OLLAMA_BASE_URL=http://localhost:11434/v1

# =============================================================================
# aX Platform Configuration
# =============================================================================

# MCP Server URL (production aX Platform)
MCP_SERVER_URL=https://mcp.paxai.app

# OAuth Server URL (production aX Platform)
OAUTH_SERVER_URL=https://api.paxai.app

# For local development with MCPJam Inspector, use:
# MCP_SERVER_URL=http://localhost:8002
# OAUTH_SERVER_URL=http://localhost:8001

# =============================================================================
# Dashboard Default Preferences
# =============================================================================

# Default agent type for new deployments
# Options: claude_agent_sdk, openai_agents_sdk, langgraph, ollama, echo
# Recommended: claude_agent_sdk (most secure, production-ready)
DEFAULT_AGENT_TYPE=claude_agent_sdk

# Default LLM provider
# Options: anthropic, openai, google, bedrock, ollama
# Recommended: anthropic (for Claude models)
DEFAULT_PROVIDER=anthropic

# Default model for the selected provider
# Anthropic: claude-sonnet-4-5, claude-haiku-4-5, claude-opus-4-5
# OpenAI: gpt-5, gpt-5-mini, gpt-5-nano, o4-mini
Copilot AI
2 days ago
The configuration section from lines 83-172 appears to be a complete duplication of the existing configuration from lines 80-82 and lines 170-172. This creates significant duplication with redundant comments and settings (DEFAULT_AGENT_TYPE, DEFAULT_PROVIDER appear multiple times). The duplicated content should be removed to maintain a single source of truth.

Copilot uses AI. Check for mistakes.
@iHelper30	Reply...
tests/stability/load_test.py
        }
        response = requests.post(f"{MOCK_URL}/workspaces/demo/messages", json=payload)
        return response.status_code == 200
    except Exception:
Copilot AI
2 days ago
The bare except Exception silently swallows all errors without logging or reporting what went wrong. This makes debugging difficult when heartbeats fail. Consider logging the exception or at least incrementing an error counter to track failure reasons.

Suggested change
    except Exception:
    except Exception as e:
        print(f"send_heartbeat failed for agent_id={agent_id}, seq={seq}: {e}", file=sys.stderr)
Copilot uses AI. Check for mistakes.
@iHelper30	Reply...
tests/stability/load_test.py
Comment on lines +59 to +74
    if total_heartbeats > 0:
        print("No errors reported.")

        # Verify messages were received
        try:
            r = requests.get(f"{MOCK_URL}/workspaces/demo/messages?since_seq=0")
            data = r.json()
            print("Sample GET after test:")
            # Print first 2 messages as sample
            sample = {"messages": data.get("messages", [])[:2]}
            print(json.dumps(sample, indent=2))
        except Exception as e:
            print(f"Failed to verify messages: {e}")
    else:
        print("Errors reported: No heartbeats sent.")
        sys.exit(1)
Copilot AI
2 days ago
The success condition logic is inverted. When total_heartbeats > 0, it prints "No errors reported" and continues, but when total_heartbeats == 0, it prints "Errors reported" and exits with error code. However, the test doesn't track actual errors - it only counts successful heartbeats. A test with 0 heartbeats could mean all requests failed OR the test duration was too short. Consider tracking and reporting actual errors separately from successful operations.

Copilot uses AI. Check for mistakes.
@iHelper30	Reply...
scripts/mcp_mock.py
import socketserver
import json
import time
from urllib.parse import urlparse, parse_qs
Copilot AI
2 days ago
Import of 'parse_qs' is not used.

Suggested change
from urllib.parse import urlparse, parse_qs
from urllib.parse import urlparse
Copilot uses AI. Check for mistakes.
@iHelper30	Reply...
scripts/register_agents.py
Comment on lines +2 to +3
import json

Copilot AI
2 days ago
Import of 'json' is not used.

Suggested change
import json
Copilot uses AI. Check for mistakes.
@iHelper30	Reply...
Merge info
Some checks were not successful
1 failing, 1 successful checks


failing checks
CLA Assistant / cla (pull_request_target)
CLA Assistant / cla (pull_request_target)Failing after 6s
successful checks
PR Pre-commit Check / Pre-commit Verification (pull_request)
PR Pre-commit Check / Pre-commit Verification (pull_request)Successful in 26s
Required
Merging is blocked
All comments must be resolved.
You're not authorized to push to this branch. Visit https://docs.github.com/repositories/configuring-branches-and-merges-in-your-repository/managing-protected-branches/about-protected-branches for more information.
Still in progress?
@iHelper30


Add a comment
Comment
 
Add your comment here...
 
Remember, contributions to this repository should follow its contributing guidelines.
 ProTip! Add comments to specific lines under Files changed.