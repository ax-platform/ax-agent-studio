# aX Agent Studio - Environment Variables
# Copy this file to .env and fill in your API keys

# =============================================================================
# LLM Provider API Keys
# =============================================================================

# Google Gemini API Key
# Get yours at: https://ai.google.dev/
GOOGLE_API_KEY=your_google_api_key_here

# Anthropic Claude API Key
# Get yours at: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# =============================================================================
# Agent Framework Preferences
# =============================================================================

# Default agent type for dashboard (echo, ollama, langgraph, claude_agent_sdk)
# This sets the initial selection when you open the dashboard
DEFAULT_AGENT_TYPE=claude_agent_sdk

# Claude Agent SDK: Use subscription instead of API key billing
# Set to 'true' to use Claude Pro/Max subscription via Claude CLI credentials
# Requires: claude login (one-time setup)
# Only affects Claude Agent SDK monitor - other monitors still use ANTHROPIC_API_KEY
USE_CLAUDE_SUBSCRIPTION=true

# =============================================================================
# Additional LLM Provider API Keys
# =============================================================================

# OpenAI API Key
# Get yours at: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# AWS Bedrock (uses AWS credentials)
# Either set these or use ~/.aws/credentials
# AWS_ACCESS_KEY_ID=your_aws_access_key
# AWS_SECRET_ACCESS_KEY=your_aws_secret_key
# AWS_REGION=us-east-1

# Set to true to enable Bedrock even without explicit AWS keys (uses local AWS config)
# AWS_BEDROCK_ENABLED=true

# =============================================================================
# Ollama Configuration (Local Models)
# =============================================================================

# Ollama runs locally, no API key needed
# Make sure Ollama is running: https://ollama.ai
OLLAMA_BASE_URL=http://localhost:11434/v1

# =============================================================================
# aX Platform Configuration
# =============================================================================

# MCP Server URL (production aX Platform)
MCP_SERVER_URL=https://mcp.paxai.app

# OAuth Server URL (production aX Platform)
OAUTH_SERVER_URL=https://api.paxai.app

# For local development with MCPJam Inspector, use:
# MCP_SERVER_URL=http://localhost:8002
# OAUTH_SERVER_URL=http://localhost:8001

# =============================================================================
# Dashboard Default Preferences
# =============================================================================

# Default agent type for new deployments
# Options: claude_agent_sdk, openai_agents_sdk, langgraph, ollama, echo
# Recommended: claude_agent_sdk (most secure, production-ready)
DEFAULT_AGENT_TYPE=claude_agent_sdk

# Default LLM provider
# Options: anthropic, openai, google, bedrock, ollama
# Recommended: anthropic (for Claude models)
DEFAULT_PROVIDER=anthropic

# Default model for the selected provider
# Anthropic: claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022, claude-3-opus-20240229
# OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo
# Google: gemini-1.5-pro, gemini-1.5-flash
# Bedrock: anthropic.claude-3-5-sonnet-20240620-v1:0, anthropic.claude-3-haiku-20240307-v1:0
# Ollama: llama3.2, qwen2.5, mistral
DEFAULT_MODEL=claude-3-5-sonnet-20241022

# Default system prompt (optional)
# Leave empty to use model defaults, or specify a prompt file from configs/prompts/
# Examples: helpful_assistant, creative_writer, code_reviewer
# DEFAULT_SYSTEM_PROMPT=

# Windows Encoding Fix
PYTHONUTF8=1
# Default environment for new deployments (optional)
# Examples: local, production, staging
# DEFAULT_ENVIRONMENT=local
